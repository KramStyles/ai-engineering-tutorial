The MNIST Database is the 'hello world' example of students who want to learn machine learning. 
The dataset includes 70,000 images of handwritten images usually containing numbers and letters.

Supervised Learning: This excels when we work with labeled data. Classification of data involves knowing what class a data belongs to e.g., providing a dataset of images of dogs and cats. When a new picture is provided, classification would help determine if it's a dog image or not. In supervised learning, we explictly trained the model with known outputs to guide its learning.

Unservised Learning: Here, we process data without labels. E.g., a large dataset of animal images containing dogs and other animals without labels. The model would have to go through all the images and surely, it would detect a pattern that would let it group each animals based on similar characteristics. 

Reinforcement Learning: This knows what the output would be just like the supervised learning but no labelled data is provided. Here rules are created and the ML model learns through trial and errors. Netflix uses this type of learning as it suggests certain shows to the user and learns based on the feedback from the user (or shows added to wishlists or skipped).

Deep Learning: This is a subset of machine learning inspired by how the human brain works. The deep learning process is complex and allows machines to learn by processing input information in stages. It has input layers, intermidiate (hidden layer(s)), and output layers.

Computer Vision: This is an AI field that uses machine learning and neural networks to teach computers and systems to derive meaningful information from digital images, videos and other visual inputs and to make recommendation or take actions when they see or detect issues. If AI functions as the brain, computer vision is the eyes.

Natural Language Processing: A field of computer science that studies how computers understand, interpret, and generate human language data.

IMPORTANT PACKAGES

nltk (3.9.1) – The Natural Language Toolkit, one of the oldest and most widely used NLP libraries. It includes tools for breaking text into words or sentences (tokenizers), simplifying words to their base form (stemmers, lemmatizers), and accessing large text collections and linguistic datasets. Great for learning and experimenting with NLP basics.

pandas (2.2.3) – Provides the DataFrame structure, which makes it easy to load, clean, and organize data in rows and columns (similar to Excel, but much more powerful). You’ll use it to manipulate text datasets and combine them with other information.

matplotlib (3.10.0) – The fundamental Python plotting library. It lets you create line charts, bar graphs, scatter plots, and more. Think of it as the "drawing canvas" for your data visualizations.

seaborn (0.13.2) – Built on top of matplotlib, but with easier functions and beautiful default styles. It’s especially good for statistical plots, like showing correlations between variables or distributions of data.

scikit-learn (1.6.0) – A general-purpose machine learning toolkit. It provides algorithms for classification, regression, clustering, feature extraction, and model evaluation. It’s often used for building traditional ML models before moving on to deep learning.

spaCy (3.8.3) – A modern NLP library designed for speed and production use. It handles advanced tasks like part-of-speech tagging (identifying nouns/verbs), named entity recognition (detecting people, places, organizations), and syntactic parsing. It’s faster and more practical than NLTK for large-scale projects.

TextBlob (0.18.0.post0) – A beginner-friendly NLP library that wraps around NLTK and other tools. It makes common tasks like sentiment analysis, phrase extraction, and translation very simple with just a few lines of code.

VADER Sentiment (3.3.2) – A lightweight, rule-based tool for sentiment analysis, tuned for short, casual texts like tweets, reviews, and comments. It understands things like exclamation marks, capitalization, and even emoji sentiment.

gensim (4.3.3) – A library for topic modeling and working with word embeddings. It’s known for algorithms like Word2Vec (which learns word meanings from context) and LDA (Latent Dirichlet Allocation) for discovering hidden topics in large text collections.

transformers (4.47.1) – From Hugging Face, this is the go-to library for using state-of-the-art NLP models like BERT, GPT, and other large language models. It makes it easy to load pretrained models for tasks like classification, text generation, and translation.

PyTorch (torch 2.5.1) – A deep learning framework that powers libraries like transformers. It’s used to build and train custom neural networks for NLP and beyond. Many cutting-edge AI models are implemented in PyTorch.

ipywidgets (8.1.5) – Provides interactive controls (like sliders, dropdowns, and buttons) inside Jupyter notebooks. These let you build small UIs that make experiments more visual and hands-on.

NumPy <(2.0.0) – The foundational library for numerical computing in Python. It introduces the powerful ndarray object, which lets you store and process large arrays and matrices of numbers efficiently. NumPy is behind almost every data science or machine learning library — it provides the fast, vectorized math operations that make Python suitable for heavy computations. You’ll use it for tasks like matrix operations, statistical calculations, and data transformations that feed into models or visualizations.

